<!DOCTYPE html>
<html lang="zh-CN" color-mode="light">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="keywords" content="" />
  <meta name="author" content="Coder109" />
  <meta name="description" content="" />
  
  
  <title>
    
      人工智能技术（一）：基础深度学习模型 
      
      
      |
    
     幻
  </title>

  
    <link rel="apple-touch-icon" href="/images/favicon.ico">
    <link rel="icon" href="/images/favicon.ico">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1886449_67xjft27j1l.css" />
  <!-- 代码块风格 -->
  

  <!-- jquery3.3.1 -->
  
    <script defer type="text/javascript" src="/plugins/jquery.min.js"></script>
  

  <!-- fancybox -->
  
    <link href="/plugins/jquery.fancybox.min.css" rel="stylesheet">
    <script defer type="text/javascript" src="/plugins/jquery.fancybox.min.js"></script>
  
  
<script src="/js/fancybox.js"></script>


  

  

  <script>
    var html = document.documentElement
    const colorMode = localStorage.getItem('color-mode')
    if (colorMode) {
      document.documentElement.setAttribute('color-mode', colorMode)
    }
  </script>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="幻" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img no-lazy src="/images/favicon.ico" alt="">
      
    </a>
    <div class="nickname"><a href="/">幻</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">Archives</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="nav-item" data-path="/friends/">
          <a href="/friends/">Friends</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->


  <!-- LaTex Display -->

  
    <script async type="text/javascript" src="/plugins/mathjax/tex-chtml.js"></script>
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    }
  </script>





  <!-- clipboard -->

  
    <script async type="text/javascript" src="/plugins/clipboard.min.js"></script>
  
  
<script src="/js/codeCopy.js"></script>







  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">人工智能技术（一）：基础深度学习模型</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
          2024-11-30 13:30:33
        </span>
        
              <span class="post-tags">
                <i class="iconfont icon-tags mr-10" title="标签"></i>
                
                <span class="span--tag mr-8">
                  <a href="/tags/AI/" title="AI">
                    #AI
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <p>本文主要讲述如何用<code>pytorch</code>编写基础的深度学习架构，并尝试用基础的架构拟合一个加法函数。</p>
<h2 id="程序代码段"><a href="#程序代码段" class="headerlink" title="程序代码段"></a>程序代码段</h2><p>基本的代码段如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">200</span></span><br><span class="line">lr = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicPlus</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.l1(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PlusDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 data_num: <span class="number">2000</span>,</span></span><br><span class="line"><span class="params">                 </span>):</span><br><span class="line">        self.data_num = data_num</span><br><span class="line">        data_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_num):</span><br><span class="line">            x = torch.rand(<span class="number">2</span>)</span><br><span class="line">            y = torch.tensor([x[<span class="number">0</span>] + x[<span class="number">1</span>]])</span><br><span class="line">            data_list.append((x, y))</span><br><span class="line">        self.data_list = data_list</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_num</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data_list[index]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># Setup datasets and dataloaders</span></span><br><span class="line">    train_dataset, val_dataset, eval_dataset = PlusDataset(<span class="number">700</span>), PlusDataset(<span class="number">100</span>), PlusDataset(<span class="number">200</span>)</span><br><span class="line">    train_loader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    val_loader = DataLoader(val_dataset, batch_size=<span class="number">32</span>)</span><br><span class="line">    eval_loader = DataLoader(eval_dataset, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Setup model and training hyperparameters</span></span><br><span class="line">    my_model = BasicPlus()</span><br><span class="line">    loss_fn = torch.nn.MSELoss()</span><br><span class="line">    optimizer = torch.optim.SGD(my_model.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        loss_value = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># Train</span></span><br><span class="line">        <span class="keyword">for</span> batch_idx, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">            <span class="comment"># Compute prediction and loss</span></span><br><span class="line">            pred = my_model(x)</span><br><span class="line">            loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Accumulate Loss for Visualization</span></span><br><span class="line">            loss_value += loss.item()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backpropagation</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss_value / <span class="built_in">len</span>(train_loader)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Validate</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            val_loss_value = <span class="number">0.0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> val_loader:</span><br><span class="line">                pred = my_model(x)</span><br><span class="line">                loss = loss_fn(pred, y)</span><br><span class="line">                val_loss_value += loss.item()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Validation Loss: <span class="subst">&#123;val_loss_value / <span class="built_in">len</span>(val_loader)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># Eval</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        eval_loss_value = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> eval_loader:</span><br><span class="line">            pred = my_model(x)</span><br><span class="line">            loss = loss_fn(pred, y)</span><br><span class="line">            eval_loss_value += loss.item()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Eval Loss: <span class="subst">&#123;eval_loss_value / <span class="built_in">len</span>(eval_loader)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Visualize the result</span></span><br><span class="line">    <span class="built_in">print</span>(my_model(torch.tensor([<span class="number">1.0</span>, <span class="number">1.0</span>])))</span><br><span class="line">    <span class="built_in">print</span>(my_model(torch.tensor([<span class="number">2.0</span>, <span class="number">2.0</span>])))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="步骤说明"><a href="#步骤说明" class="headerlink" title="步骤说明"></a>步骤说明</h2><p>这是一个用<code>pytorch</code>搭建的基础的深度学习代码框架。所有的深度学习流程，都遵循着这样的步骤：</p>
<ol>
<li>数据预处理。</li>
<li>模型加载。</li>
<li>训练流程的编写。</li>
<li>模型的测试与保存。</li>
</ol>
<h2 id="预备知识：张量"><a href="#预备知识：张量" class="headerlink" title="预备知识：张量"></a>预备知识：张量</h2><p><code>pytorch</code>中，最基本的数据是<code>tensor</code>——张量。<code>pytorch</code>中的张量与其他领域不同，您可以将之理解为<strong>标量、矩阵以及矩阵之矩阵的集合</strong>。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(<span class="number">1</span>) <span class="comment"># 这是一个标量式的张量，可以看作一个数字（标量），无形状可言</span></span><br><span class="line">torch.tensor([<span class="number">1</span>,<span class="number">2</span>]) <span class="comment"># 这是一个矩阵式的张量，可以看作一个一行二列的矩阵，形状为3</span></span><br><span class="line">torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]) <span class="comment"># 这是一个矩阵式的张量，可以看作一个二行三列的矩阵，形状为2x3 </span></span><br><span class="line">torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]]) <span class="comment"># 这是一个矩阵之矩阵，可以看作一个一行一列的矩阵，成员是一个二行三列的矩阵，形状为1x2x3</span></span><br></pre></td></tr></table></figure>



<p>您可以通过<code>Tensor.shape</code>来查看一个张量的形状。</p>
<p>也可以将其理解为，<strong>标量、列表、等长列表之列表的集合。</strong>这是从数据组织的角度，而非计算角度来理解张量。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>深度学习的数据通常按照如下的方法组织：<strong>（输入，预期输出）</strong>。数据集的质量直接影响着模型的表现。</p>
<p>作为示例，我们所使用的数据集是代码生成的，在应用<code>pytorch</code>编码时，我们首先会将以其他形式组织的数据集，变化为<code>torch.utils.data.Dataset</code>的子类，这个类是将各种各样数据集做规范化的一个类。</p>
<p>作为子类，我们通常要重写这样三个方法：<code>__init__()</code>、<code>__len__()</code>和<code>__getitem__()</code>。</p>
<p><code>__init__()</code>是构造器，我们通常在这个方法中对数据做一些预处理，将数据变为（输入，预期输出）这样的格式，并将全部数据转化为可以迭代的结构，便于后续的迭代操作。在上述的示例代码中，我们在这个方法里完成了：导入数据、将数据组织为列表这两个功能。</p>
<p><code>__len__()</code>方法，需要返回数据集的大小。</p>
<p><code>__getitem__()</code>方法，则需要完成这样的功能：给定索引，返回数据集中对应的子数据。</p>
<p>数据预处理的关键在于预处理，通常，在代码编写完毕后，我们要print一些处理后的数据，观察它们的数据、形状是否与预期相符。</p>
<p>在加载数据为<code>Dataset</code>之后，我们还需要一个<code>DataLoader</code>来将<code>Dataset</code>进一步处理。<code>DataLoader</code>能够完成包括但不限于这样几个工作：对数据集进行分组、打乱数据集顺序等。在实际的训练过程中，如果从<code>Dataset</code>里，每次挑出一个来训练，会很慢，所以训练通常是按照批次（batch）来训练的，<code>DataLoader</code>就完成了这样一个工作。</p>
<h2 id="模型加载：构造器"><a href="#模型加载：构造器" class="headerlink" title="模型加载：构造器"></a>模型加载：构造器</h2><p>我们通常会借助<code>torch.nn.Module</code>，实现我们的模型。</p>
<p>我们通常会重写它的两个方法：<code>__init__()</code>和<code>forward()</code>。</p>
<p>在构造器<code>__init__()</code>中，我们会声明模型的架构，即内部包含哪些层。在我们的示例代码中，我们在构造器里定义了一个线性层（Linear Layer），所谓线性层，实际上功能和这样一个函数类似：<br>$$<br>\mathbf{y}&#x3D;\mathbf{Wx}+\mathbf{b}<br>$$</p>
<p><code>torch.nn.Linear(2,1)</code>的意思就是，声明上面公式中的$\mathbf{W}$是一个2$\times$1的矩阵。这个2和1代表的是<code>in_features</code>与<code>out_features</code>。所谓<code>feature</code>就是输入的<strong>特征</strong>的数量，如果一个数据，能被表征成<code>torch.tensor([1,2,3])</code>的形式，其特征有3个。</p>
<p>不过，和线性代数中不同的是，线性层可以对多个数据同时做操作。还记得我们之前说过的<code>DataLoader</code>吗，我们通常用模型+<code>DataLoader</code>的形式进行训练。两个数据组织成<code>torch.tensor([[1,2,3], [1,2,3]])</code>的形式，那么，线性层可以对这两个数据同时处理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_layer = torch.nn.Linear(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">test_layer(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)) <span class="comment"># 返回tensor([-1.1655], grad_fn=&lt;ViewBackward0&gt;)</span></span><br><span class="line">test_layer(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]], dtype=torch.<span class="built_in">float</span>)) <span class="comment"># 返回tensor([[-1.1655],[-3.2795]], grad_fn=&lt;AddmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure>



<p>如果您本地的结果和我不一样，无需担心，因为每次线性层初始化，其权重都会随机生成。</p>
<h2 id="模型加载：前向传播"><a href="#模型加载：前向传播" class="headerlink" title="模型加载：前向传播"></a>模型加载：前向传播</h2><p>我们来讨论一下<code>forward()</code>方法。这个方法负责<strong>前向传播</strong>。前向传播过程，实际上就是推理过程。您需要在这个方法中编写处理输入的逻辑。依靠在<code>__init__()</code>中预先设置好的层，对输入一步步进行处理。我们的示例代码很简单，其<code>forward()</code>方法实现了这样一个功能，对于输入，经过线性层处理，然后输出。</p>
<h2 id="训练流程编写"><a href="#训练流程编写" class="headerlink" title="训练流程编写"></a>训练流程编写</h2><p>到目前为止，我们加载好了数据集，也编写了模型架构，让我们开始编写训练代码！</p>
<p>训练流程通常为：</p>
<ol start="0">
<li><p>初始化数据与模型。</p>
</li>
<li><p>迭代数据。</p>
</li>
<li><p>依靠模型，生成对输入的预测值。</p>
</li>
<li><p>计算预测值与预期的误差。</p>
</li>
<li><p>依靠优化器对模型权重进行优化、更新。</p>
</li>
</ol>
<h3 id="第零步：初始化"><a href="#第零步：初始化" class="headerlink" title="第零步：初始化"></a>第零步：初始化</h3><p>这里我们主要谈<code>DataLoader</code>的设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>这里的<code>batch_size</code>是批次的大小，即模型同时对多少个数据进行处理，而<code>shuffle</code>参数则表示我们要对数据集进行打乱，我们通常对训练集进行打乱，原因见<a href="#%E9%99%84%E5%BD%95">附录</a>。</p>
<h3 id="第一步：迭代数据"><a href="#第一步：迭代数据" class="headerlink" title="第一步：迭代数据"></a>第一步：迭代数据</h3><p>我们有时会将<code>DataLoader</code>组织成一个枚举类，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch_idx, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br></pre></td></tr></table></figure>

<p>因为有时，我们需要获取数据的下标信息。</p>
<h3 id="第二步：生成预测"><a href="#第二步：生成预测" class="headerlink" title="第二步：生成预测"></a>第二步：生成预测</h3><p>由于模型通常已经写好了<code>__call__()</code>方法，直接用<code>model(x)</code>类似的代码段即可。</p>
<h3 id="第三步：计算误差"><a href="#第三步：计算误差" class="headerlink" title="第三步：计算误差"></a>第三步：计算误差</h3><p>我们会预先设置一些损失函数，来计算误差。损失函数的选择通常是任务敏感的。比如分类任务常用交叉熵损失，预测任务常用均方误差损失。我们想实现一个加法器，那么就是一个预测任务，选用均方误差比较好，其公式为：<br>$$<br>MSE(T)&#x3D;E((T-\theta)^2)&#x3D;\frac{1}{n}\sum_{i&#x3D;1}^n(y_i-\hat{y}_i)^2<br>$$<br>选用L1误差函数等也都可以。</p>
<h3 id="第四步：优化"><a href="#第四步：优化" class="headerlink" title="第四步：优化"></a>第四步：优化</h3><p>优化器是一个根据loss计算梯度，优化模型权重的模块。</p>
<p>为什么要优化？当然是为了loss最低。这是一个极其显然的想法。此外，我们也知道，现在可以改变的东西，只有模型的权重，所以我们的任务变成了：如何寻找到权重-损失函数的鞍点，横轴是权重，纵轴是损失。怎么找？梯度下降法。</p>
<p>假设我们的权重只有一个，而损失函数恰好满足$y&#x3D;x^2$这个函数：</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAMAAABOo35HAAAABGdBTUEAAK/INwWK6QAAABl0RVh0U29mdHdhcmUAQWRvYmUgSW1hZ2VSZWFkeXHJZTwAAAC9UExURVlZWdPT07KysmRkZIWFhfT09JmZmWZmZm9vb39/fxkZGUxMTDMzM3p6epCQkKamppubm729venp6cjIyN7e3tbW1s/Pz8LCwnx8fLS0tFZWVoiIiI+Pj6GhoeTk5Glpabu7u93d3evr66CgoJSUlKqqqsnJyeDg4Hd3d8PDw+Xl5bi4uNHR0dvb26Ojo6urq+fn51hYWDg4OCgoKHBwcK2traenp0FBQe7u7vHx8U5OTre3t8zMzHV1df///7GrnpQAAAA/dFJOU///////////////////////////////////////////////////////////////////////////////////AI4mfBcAAAUGSURBVHja7NoJb6M4GMZxY0NCD64kve/pMZ2d3Z297+X7f6zFNmBAMUXa6URl/q9UJSWPUPzrizFWRUlNLgEBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYYIEFAVhggQUWWGBBABZYYIEFFlgQgAUWWGCBBRYEYIEFFlhggQUBWGCBBRZYYEEAFlhggQUWWBCABRZYYIEFFgRggQUWWGCBBQFYYIEFFlhgQQAWWGCBBRZYEIAFFlhggQUWBGCBBRZYn6cCIcRXgvX/h9qcIVBqDdbEM8RCxGCB9QqXYRwHYDHBgwXWl8eKZKiESHI3Ba1kWs3fKixcaJUl1YyeBm7Ocq+yLItUiVBGnXxenSHJolIKEcwHq6ikbOX1YGVzQCTN8LPmSLreghUl9sN4Uw7yajMrLC0TZ1ImzqY6FEop0+pIaEN5HaoOxVuwEqFyc4I46uSlzOLqgxlh6UaR9l3VYWl9Fdoxb1Q90KJtu41pwwFW/WHhTtW8i7TafLCqRsk6bsGw63L9qurXRmuIlbT9lDQnlXU+nBFW1Q2qnZbDprWa2tjR90LZFqx1/+Td/HpGWLlrLDvIwTcx6dQ1Vrntbig68cDms3JwbA5Y1azs1ger6sNV/bbIw1jU81MvNAGrl58RVn8ozW+btF08iGFoAlYvP3csfVur1gJBEIA1uBmue5dhZDOyO2epbmgCVi8/I6x0MMHH9pjsTfBhNzQBq5uPZoQlB0uH3DZG4EZqQ26fL3sZq5uf09Ih6qw3i/pm6BZO0qZX7rrUS68Xsbr5ZE4rePMk08pk9aUZugfqppvs6AM1Acvlo/StP+6EbW06z8hJqxbYp2BZPQUnFsLsKuhQdaHqn5ewbF7KXIn0jWO5MqOQ7RaNLPtbNMmmhimj0GUmYLl8Gs0Lq4wyPbTu1l2QKqHSouzs3OlDIslW5SQsnY/NXmFplyNvEuuLV/Tau9BzwiraDUSwXmysztYWWNtL1psXeumgIrDGaqXvBfUuvtqUYI3V2t1wk1e2msFluJJm6zDJXv/fIfjPP7DAAgsssCiwwAILLLDAosACCyywwAKLAgsssMACC6zt9fDz/v75tyOB+98PD2+ORgKffjw4OP1uJPDxl+Xy8v1I4MPF3t7VNyOB4/vF4uzdzrG+39f1kz/w66Guv/yBvw90KX/gZKkr8Qf+2dOV+gNHC12/7RxrabD2/a31bLAO/a11YbAO/K21MFhLf2s9Gqw9f2vdGqzFu11jnVusE2/gxmI9eQOnFuvYG7i0WH7uK4t15w2cWazrXWP9a7H8f/bQYvm/6IPF+sF/pVssf19Ii/WH/0K2WH/uGuvEWC39gSdj9Twy+Rqri5EZx1gt/IE7Y/XoD1wbq9vd3w1PlufnD2OBp+ebm/uxwPHF6emnscDR4vLy41jg7vHq6sNY4Pr27OyYdRaLUrDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssMCiwAILLLDAAosCCyywwAILLAossMACCyywKLDAAgsssL6u+k+AAQCR9eHtLKvLfwAAAABJRU5ErkJggg==" data-original="/2024/11/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/Figure_1.png"></p>
<p>那么我们作这样一张图：</p>
<p><img src="data:image/png;base64,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" data-original="/2024/11/30/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B/example.png"></p>
<p>假设我们现在的函数在黑橙交点（这个点也是函数线上的点），为了优化，我们需要<strong>左移</strong>，左移的步长是橘红交点左侧红线的长度，这就引出了我们第一个公式：<br>$$<br>\exist \lambda, F(x-\lambda\nabla F(x_0))&lt;F(x_0)<br>$$<br>其中，$x_0$就是我们的黑橙交点。这个$\lambda$就是学习率(learning rate，代码中的$lr$)。</p>
<p>优化器会根据学习率的大小，寻找到一个最优秀的参数设置。不过我们也能看出来，如果学习率太大，移动的步长就会很大，这会导致参数设置在极值点的两侧来回震荡；而学习率太小，移动的步长太小，这会导致训练很慢。</p>
<p>代码中，我们首先要将优化器的梯度设置为0，然后计算loss对于参数的梯度，最后应用优化器进行优化，这便是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

<p>三行的作用。</p>
<h2 id="测试流程编写"><a href="#测试流程编写" class="headerlink" title="测试流程编写"></a>测试流程编写</h2><p>测试的时候，我们不需要模型更新参数，所以需要指定：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad()</span><br></pre></td></tr></table></figure>

<p>这表示，我们不需要梯度计算，因为梯度计算只和参数的优化有关，关闭它可以加快推理过程。</p>
<p>测试过程中，我们只需要让模型前向传播，然后得出loss即可。我们通常使用平均损失来观察模型的效果。</p>
<h2 id="视觉化-Visualization-的重要性"><a href="#视觉化-Visualization-的重要性" class="headerlink" title="视觉化(Visualization)的重要性"></a>视觉化(Visualization)的重要性</h2><p>视觉化是指将训练过程中的某些参数显示出来，这是很重要的。很多运算都是计算机自动完成的，不会显示出来，这不利于我们观测训练的过程。</p>
<p>常见的视觉化方法有这么几个：对于训练过程，可以应用<code>tqdm</code>设置进度条，观测训练的耗时；对于训练效果，可以设置训练几个<code>epoch</code>后，在验证集上进行推理，观察效果……</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>基础深度学习模型的代码编写，有这样几个重点：</p>
<ol>
<li>数据的预处理：将数据进行预先处理，并转化为可以迭代的结构。</li>
<li>模型的编写：模型内部包含哪些模块？前向传播该怎么做？</li>
<li>训练过程的编写：正确选择损失函数、优化器，设置参数。</li>
<li>视觉化：设置合适的视觉化方式，便于用户观测训练过程。</li>
</ol>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="训练集、验证集、测试集"><a href="#训练集、验证集、测试集" class="headerlink" title="训练集、验证集、测试集"></a>训练集、验证集、测试集</h3><p>训练集是用于训练、优化模型参数的集合；验证集是在训练过程中，用于推理，检测模型临时训练效果的集合；测试集则是在训练后，单纯用于推理，检测模型训练效果的集合。</p>
<h3 id="Shuffle的必要性"><a href="#Shuffle的必要性" class="headerlink" title="Shuffle的必要性"></a>Shuffle的必要性</h3><p>假设我们的权重-损失函数有三个鞍点，其中只有一个是最低点。如果不shuffle，假设第一个epoch训练完毕，模型权重到达了一个局部最低点，而非极低点，那么后续的训练有可能无法跳出这个点。为了避免这个，我们通常会打乱数据集，创造更多的数据组合。</p>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2024/07/30/%E5%BE%B7%E5%8B%92%E5%85%B9%EF%BC%9A%E6%97%A0%E4%B8%AD%E5%BF%83%E7%9A%84%E6%B8%B8%E7%89%A7/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>上一页</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime mr-10" title="更新时间"></i>
              2024-11-30 13:30:33
            </span>
            
                  <span class="post-tags">
                    <i class="iconfont icon-tags mr-10" title="标签"></i>
                    
                    <span class="span--tag mr-8">
                      <a href="/tags/AI/" title="AI">
                        #AI
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/2024/12/07/%E3%80%90%E4%B9%90%E7%90%86%E3%80%91%E5%9F%BA%E7%A1%80%E7%9A%84%E4%B9%90%E7%90%86%E6%95%99%E7%A8%8B/" target="_self">
                <span>下一页</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div id="btn-catalog" class="btn-catalog">
    <i class="iconfont icon-catalog"></i>
  </div>
  <div class="post-catalog hidden" id="catalog">
    <div class="title">目录</div>
    <div class="catalog-content">
      
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81%E6%AE%B5"><span class="toc-text">程序代码段</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A5%E9%AA%A4%E8%AF%B4%E6%98%8E"><span class="toc-text">步骤说明</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86%EF%BC%9A%E5%BC%A0%E9%87%8F"><span class="toc-text">预备知识：张量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%EF%BC%9A%E6%9E%84%E9%80%A0%E5%99%A8"><span class="toc-text">模型加载：构造器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%EF%BC%9A%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">模型加载：前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E7%BC%96%E5%86%99"><span class="toc-text">训练流程编写</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E9%9B%B6%E6%AD%A5%EF%BC%9A%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">第零步：初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E8%BF%AD%E4%BB%A3%E6%95%B0%E6%8D%AE"><span class="toc-text">第一步：迭代数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E7%94%9F%E6%88%90%E9%A2%84%E6%B5%8B"><span class="toc-text">第二步：生成预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E8%AF%AF%E5%B7%AE"><span class="toc-text">第三步：计算误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E4%BC%98%E5%8C%96"><span class="toc-text">第四步：优化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95%E6%B5%81%E7%A8%8B%E7%BC%96%E5%86%99"><span class="toc-text">测试流程编写</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%86%E8%A7%89%E5%8C%96-Visualization-%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">视觉化(Visualization)的重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-text">附录</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-text">训练集、验证集、测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="toc-text">Shuffle的必要性</span></a></li></ol></li></ol>
      
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
  </div>


        
<div class="footer">
  <div class="social">
    <ul>
      
        <li>
          <a title="rss" href="/atom.xml">
            <i class="iconfont icon-rss"></i>
          </a>
        </li>
      
    </ul>
  </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Copyright © 2024 Oranges</a>
        
    </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
        
    </div>
  
  
</div>

      </div>

      <div class="tools-bar">
        <div class="back-to-top tools-bar-item hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



        
  <div class="search-icon tools-bar-item" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>首次搜索，正在载入索引文件，请稍后……<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>没有找到内容，请尝试更换检索词。<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        },
        error: function(xhr, status, error) {
          $resultContent.innerHTML = ""
          if (xhr.status === 404) {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>未找到search.xml文件，具体请参考：<a href='https://github.com/zchengsite/hexo-theme-oranges#configuration' target='_black'>configuration</a><span></ul>";
          } else {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>请求失败，尝试重新刷新页面或稍后重试。<span></ul>";
          }
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




        
  <div class="tools-bar-item theme-icon" id="switch-color-scheme">
    <a href="javascript: void(0)">
      <i id="theme-icon" class="iconfont icon-moon"></i>
    </a>
  </div>

  
<script src="/js/colorscheme.js"></script>





        
  
    <div class="share-icon tools-bar-item">
      <a href="javascript: void(0)" id="share-icon">
        <i class="iconfont iconshare"></i>
      </a>
      <div class="share-content hidden">
        
          <a class="share-item" href="https://twitter.com/intent/tweet?text=' + %E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%9F%BA%E7%A1%80%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B + '&url=' + https%3A%2F%2Fcoder109.github.io%2F2024%2F11%2F30%2F%25E4%25BA%25BA%25E5%25B7%25A5%25E6%2599%25BA%25E8%2583%25BD%25E6%258A%2580%25E6%259C%25AF%25EF%25BC%2588%25E4%25B8%2580%25EF%25BC%2589%25EF%25BC%259A%25E5%259F%25BA%25E7%25A1%2580%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E6%25A8%25A1%25E5%259E%258B%2F + '" target="_blank" title="Twitter">
            <i class="iconfont icon-twitter"></i>
          </a>
        
        
      </div>
    </div>
  
  
<script src="/js/shares.js"></script>



      </div>
    </div>
  
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body>
</html>
